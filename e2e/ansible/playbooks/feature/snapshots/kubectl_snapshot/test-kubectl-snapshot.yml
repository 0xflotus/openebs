#test-kubectl-snapshot.yml
# Description: Test backup and restore operations with mysql application using k8s snapshot api.

###############################################################################################
#Test Steps:

#1. Check if openebs components are running.
#2. Copy the test artifacts to k8s master.
#3. Deploy the snapshot controller.
#4. Deploy Percona application.
#5. Check if the application pod is up and running
#6. Create test database inside the percona pod.
#7. Create volume snapshot.
#8. Create storage class.
#9. Restore the snapshot.
#10. Check if the sync happened successfully between the volumes.
#11. Deploy the application mapped with new pvc.
#12. Check if the written table data resides inside the new pod.
#13. Perform cleanup of test artifacts.
###############################################################################################

---
- hosts: localhost

  vars_files:
    - kubectl-snapshot-vars.yml

  tasks:

    - block:

        - include: prerequisites.yml

        - name: 1) Check if openebs-provisioner and maya-apiserver are running
          shell: source ~/.profile; kubectl get pods | grep Running
          args:
            executable: /bin/bash
          register: pods_list
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          until: "'openebs-provisioner' and 'maya-apiserver' in pods_list.stdout"
          delay: 60
          retries: 5

        - name: 2) Copy the snapshot specific files to K8s master
          copy:
            src: "{{ item }}"
            dest: "{{ result_kube_home.stdout }}"
          with_items: "{{ snapshot_files }}"
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: 3) Deploy Snapshot controller
          shell: source ~/.profile; kubectl create -f "{{ snapshot_operator }}"
          args:
            executable: /bin/bash
          register: operator_output
          until: "'deployment \"snapshot-controller\" created' in operator_output.stdout"
          delay: 60
          retries: 10
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: 3a) Check if snapshot-controller is running.
          shell: source ~/.profile; kubectl get pods --all-namespaces | grep snapshot-controller
          args:
            executable: /bin/bash
          register: sp_ctrl
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          until: "'Running' in sp_ctrl.stdout"
          delay: 60
          retries: 10

        - name: 4) Deploy percona mysql pod
          shell: source ~/.profile; kubectl create -f "{{ percona_link }}"
          args:
            executable: /bin/bash
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: 5) Confirm pod status is running
          shell: source ~/.profile; kubectl get pods | grep percona
          args:
            executable: /bin/bash
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          register: result
          until: "'percona' and 'Running' in result.stdout"
          delay: 120
          retries: 15

        - name: 5a) Get IP address of percona mysql pod
          shell: source ~/.profile; kubectl describe pod percona
          args:
            executable: /bin/bash
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          register: result_IP

        - name: 5b) Set IP of Pod to variable
          set_fact:
            pod_ip: "{{ result_IP.stdout_lines[7].split()[1] }}"

        - name: 6) Write a test database into percona mysql
          shell: |
            sleep 120 #db init time
            kubectl exec -it percona -- mysql -uroot -pk8sDem0 -e "create database tdb;"
            kubectl exec -it percona -- mysql -uroot -pk8sDem0 -e "create table ttbl (Data VARCHAR(20));" tdb
            kubectl exec -it percona -- mysql -uroot -pk8sDem0 -e "insert into ttbl (Data) VALUES ('tdata');" tdb
            kubectl exec -it percona -- mysql -uroot -pk8sDem0 -e "flush tables with read lock;"
          args:
            executable: /bin/bash
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          register: db_result
          failed_when: "db_result.rc != 0"

        - name: 6a) Perform fs sync on percona pod before taking snap
          shell: >
            source ~/.profile;
            kubectl exec "{{pod_name}}" -- bash -c "sync;sync;sync"
          args:
            executable: /bin/bash
          register: result_snap
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: 7) Creating the volume snapshot
          shell: source ~/.profile; kubectl create -f "{{ snapshot }}"
          args:
            executable: /bin/bash
          register: sp
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          until: '"volumesnapshot \"snapshot-demo\" created" in sp.stdout'
          delay: 60
          retries: 5

        - name: 7a) Check if the snapshot is created successfully
          shell: sleep 20 ; source ~/.profile; kubectl describe volumesnapshot snapshot-demo | grep Message
          args:
            executable: /bin/bash
          register: sp_out
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          until: "'Snapshot created successfully' in sp_out.stdout"
          delay: 60
          retries: 5

        - name: 8) Creating the snapshot-promoter storage class
          shell: source ~/.profile; kubectl create -f "{{ snapshot_sc }}"
          args:
            executable: /bin/bash
          register: sc
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          until: "'storageclass \"snapshot-promoter\" created' in sc.stdout"
          delay: 30
          retries: 5

        - name: 9) Creating PVC from the snapshot
          shell: source ~/.profile; kubectl create -f "{{ snapshot_claim }}"
          args:
            executable: /bin/bash
          register: claim_out
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          until: "'persistentvolumeclaim \"demo-snap-vol-claim\" created' in claim_out.stdout"
          delay: 60
          retries: 5

        - name: 9a) Checking if the pvc is created successfully
          shell: source ~/.profile; kubectl get pvc | grep demo-snap-vol-claim
          args:
            executable: /bin/bash
          register: pvc_out
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          until: "'Bound' in pvc_out.stdout"
          delay: 60
          retries: 10

        - name: 10) Obtaining new PV name
          shell: source ~/.profile; kubectl get pv | grep demo-snap-vol-claim | awk {'print $1'}
          args:
            executable: /bin/bash
          register: snap_pv
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: 10a) Obtaining the new controller pod name
          shell: source ~/.profile; kubectl get po | grep {{ snap_pv.stdout }} | grep ctrl | awk {'print $1'}
          args:
            executable: /bin/bash
          register: new_ctrl_pod
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: 10b) Checking if the sync completed
          shell: source ~/.profile; kubectl logs {{new_ctrl_pod.stdout}}
          args:
            executable: /bin/bash
          register: ctrl_log
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          until: "'Done synchronizing' in ctrl_log.stdout"
          delay: 100
          retries: 10

       # - name: Wait few seconds
       #   wait_for: timeout=60
       #   delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: 10c) Check if the ctrl and replica pods are running
          shell: kubectl get pods | grep {{snap_pv.stdout}} | grep "{{ item }}"
          args:
            executable: /bin/bash
          register: pod_status
          until: "'Running' in pod_status.stdout"
          delay: 100
          retries: 15
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          with_items:
            - ctrl
            - rep

        - name: 11) Copy the percona yaml to K8s master
          copy:
            src: "{{ percona }}"
            dest: "{{ result_kube_home.stdout }}"
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: 11a) Deploying the application with the snapped pvc
          shell: source ~/.profile; kubectl create -f "{{ percona }}"
          args:
            executable: /bin/bash
          register: new_app
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          until: '"pod \"percona-new\" created" in new_app.stdout'
          delay: 120
          retries: 5

        - name: Wait few seconds
          wait_for: timeout=120
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: 11b) Check if new percona pod has been running successfully
          shell: >
            source ~/.profile;
            kubectl get pods -l name=percona-new --no-headers
          args:
            executable: /bin/bash
          register: app_pod
          until: "'Running' in app_pod.stdout"
          delay: 120
          retries: 5
          delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

        - name: 11c) Get new percona pod details
          shell: >
            source ~/.profile;
            kubectl get pods -o wide | grep percona-new
          args:
            executable: /bin/bash
          register: new_pod
          delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

        - name: 11d) Store new percona pod IP in variable
          set_fact:
            new_pod_ip: "{{ new_pod.stdout.split()[5] }}"

        - name: 12) Verify successful snapshot restore by db query
          shell: |
            sleep 120 # DB init
            kubectl exec -it percona -- mysql -uroot -pk8sDem0 -e "select * from ttbl;" tdb
           # mysql -uroot -pk8sDem0 -h {{new_pod_ip}} -e "select * from ttbl;" tdb
          args:
            executable: /bin/bash
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          register: db_result
          failed_when: "'tdata' not in db_result.stdout"

        - name: Set the pass flag
          set_fact:
            flag: "Pass"

      rescue:
        - name: set the fail flag
          set_fact:
            flag: "Fail"

      always:

        - name: Wait few seconds before initiating cleanup
          wait_for: timeout=60
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: 13) Cleaning the test artifacts
          include: kubectl-snapshot-cleanup.yml

        - name: Terminate the log aggregator
          shell: source ~/.profile; killall stern
          args:
            executable: /bin/bash
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: Send slack notification
          slack:
            token: "{{ lookup('env','SLACK_TOKEN') }}"
            msg: '{{ ansible_date_time.time }} TEST: {{test_name}}, RESULT: {{ flag }}'
          when: slack_notify | bool and lookup('env','SLACK_TOKEN')


