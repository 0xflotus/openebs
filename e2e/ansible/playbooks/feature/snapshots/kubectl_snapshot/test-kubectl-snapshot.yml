#kubectl-snapshot.yml
# Description: Test openebs snapshots using kubectl.

###############################################################################################
#Test Steps:

#1. Check whether the OpenEBS components are deployed. Delete the Openebs components and create with ci tag.
#2. Copy the test artifacts to k8s master.
#3. Deploy the snapshot controller.
#3. Deploy Percona application.
#4. Check if the application pod is up and running
#5. Check if the Percona service us up.
#6. Calculate the checksum.
#7. Create snapshot.
#8. Create storage class.
#9. Restore the snapshot.
#10. Deploy the application mapped with new pvc
#11. Check the data
#12. Perform cleanup of test artifacts.
###############################################################################################

---
- hosts: localhost

  vars_files:
    - kubectl-snapshot-vars.yml

  tasks:
   - block:

      # Deleting openebs components currently. Once the PR is merged, there is no need to delete and recreate.
       - name: 1) Delete Openebs components
         shell: source ~/.profile; kubectl delete -f {{ openebs_link }}
         args: 
           executable: /bin/bash
         register: delete_openebs
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: "'deleted' in delete_openebs.stdout"
         delay: 20
         retries: 5

       - name: 1a) Check if openebs-provisioner and maya-apiserver are deleted
         shell: kubectl get pods 
         args:
           executable: /bin/bash
         register: pod_list
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: "'openebs-provisioner' and 'maya-apiserver' not in pod_list.stdout"
         delay: 60
         retries: 5

       - name: 1c) Get $HOME of K8s master for kubernetes user
         shell: source ~/.profile; echo $HOME
         args:
           executable: /bin/bash
         register: result_k8s_home
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 1d) Copy the openebs operator file with ci image to K8s master
         copy:
           src: "{{ openebs_ci }}"
           dest: "{{ result_k8s_home.stdout }}"
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 1e) Deploy OpenEBS with ci image
         shell: source ~/.profile; kubectl apply -f {{ openebs_ci }}
         args:
           executable: /bin/bash
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
 
       - name: 1f) Check if openebs-provisioner and maya-apiserver are running
         shell: kubectl get pods | grep Running
         args:
           executable: /bin/bash
         register: pods_list
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: "'openebs-provisioner' and 'maya-apiserver' in pods_list.stdout"
         delay: 60
         retries: 15
         
       - name: 2) Copy the snapshot specific files to K8s master
         copy:
           src: "{{ item }}"
           dest: "{{ result_k8s_home.stdout }}"
         with_items: "{{ snapshot_files }}"
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 3) Deploy Snapshot controller
         shell: source ~/.profile; kubectl create -f "{{ snapshot_operator }}"
         args: 
           executable: /bin/bash
         register: operator_output
         until: "'deployment \"snapshot-controller\" created' in operator_output.stdout"
         delay: 60
         retries: 5
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 3a) Check if snapshot-controller is running.
         shell: kubectl get pods --all-namespaces | grep snapshot-controller
         args:
           executable: /bin/bash
         register: sp_ctrl
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: "'Running' in sp_ctrl.stdout"
         delay: 60
         retries: 5

       - name: 4) Deploy percona mysql pod
         shell: source ~/.profile; kubectl create -f {{ percona_link }}
         args:
           executable: /bin/bash
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 5) Confirm pod status is running
         shell: source ~/.profile; kubectl get pods | grep percona
         args:
           executable: /bin/bash
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         register: result
         until: "'percona' and 'Running' in result.stdout"
         delay: 60
         retries: 15

       - name: 5a) Get IP address of percona mysql pod
         shell: source ~/.profile; kubectl describe pod percona
         args:
           executable: /bin/bash
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         register: result_IP

       - name: 5b) Set IP of Pod to variable
         set_fact:
           pod_ip: "{{ result_IP.stdout_lines[7].split()[1] }}"

       - name: 5c) Write a test database into percona mysql
         shell: |
           mysql -uroot -pk8sDem0 -h {{pod_ip}} -e "create database tdb;"
           mysql -uroot -pk8sDem0 -h {{pod_ip}} -e "create table ttbl (Data VARCHAR(20));" tdb
           mysql -uroot -pk8sDem0 -h {{pod_ip}} -e "insert into ttbl (Data) VALUES ('tdata');" tdb
           mysql -uroot -pk8sDem0 -h {{pod_ip}} -e "flush tables with read lock;"
         args:
           executable: /bin/bash
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         register: result
         failed_when: "result.rc != 0"

       - name: 6) Get $HOME of K8s minion for kubernetes user
         shell: source ~/.profile; echo $HOME
         args:
           executable: /bin/bash
         register: result_kube_home
         delegate_to: "{{groups['kubernetes-kubeminions'].0}}"

       - name: 9) Creating the volume snapshot
         shell: kubectl create -f {{ snapshot }}
         args:
           executable: /bin/bash
         register: sp
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: '"volumesnapshot \"snapshot-demo\" created" in sp.stdout'
         delay: 30
         retries: 5

       - name: 9a) Check if the snapshot is created successfully
         shell: source ~/.profile; kubectl describe volumesnapshot snapshot-demo | grep Message
         args:
           executable: /bin/bash
         register: sp_out
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: "'Snapshot created successfully' in sp_out.stdout"
         delay: 30
         retries: 5

       - name: 10) Creating the snapshot-promoter storage class
         shell: source ~/.profile; kubectl create -f {{ snapshot_sc }}
         args:
           executable: /bin/bash
         register: sc
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: "'storageclass \"snapshot-promoter\" created' in sc.stdout"
         delay: 30
         retries: 5

       - name: 11) Creating the PVC from the snapshot
         shell: source ~/.profile; kubectl create -f {{ snapshot_claim }}
         args:
           executable: /bin/bash
         register: claim_out
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: "'persistentvolumeclaim \"demo-snap-vol-claim\" created' in claim_out.stdout"
         delay: 30
         retries: 5

       - name: 11a) Checking if the pvc is created successfully
         shell: source ~/.profile; kubectl get pvc | grep demo-snap-vol-claim
         args:
           executable: /bin/bash
         register: pvc_out
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: "'Bound' in pvc_out.stdout"
         delay: 30
         retries: 5
     
       - name: 12) Copy the openebs operator file with ci image to K8s master
         copy:
           src: "{{ percona }}"
           dest: "{{ result_k8s_home.stdout }}"
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 12a) Deploying the application with the snapped pvc
         shell: source ~/.profile; kubectl create -f {{ percona }}
         args:
           executable: /bin/bash
         register: new_pvc
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: '"pod \"percona-new\" created" in new_pvc.stdout'
         delay: 20
         retries: 5

       - name: 13) Check if percona pod restarted successfully
         shell: >
           source ~/.profile;
           kubectl get pods -l name=percona --no-headers
         args:
           executable: /bin/bash
         register: result
         until: "'Running' in result.stdout"
         delay: 10
         retries: 6
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

       - name: 13a) Get new percona pod details
         shell: >
           source ~/.profile;
           kubectl get pods -o wide | grep percona-new
         args:
           executable: /bin/bash
         register: new_pod
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

       - name: 13b) Store new percona pod IP in variable
         set_fact:
           new_pod_ip: "{{ new_pod.stdout.split()[5] }}"

       - name: 13c) Verify successful snapshot restore by db query
         shell: >
           mysql -uroot -pk8sDem0 -h {{new_pod_ip}}
           -e "select * from ttbl;" tdb
         args:
           executable: /bin/bash
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         register: db_result
         failed_when: "'tdata' not in db_result.stdout"

       - set_fact:
           flag: "Pass"

     rescue:
       - set_fact:
           flag: "Fail"

     always:

       - name: Cleaning the test artifacts
         include: kubectl-snapshot-cleanup.yml
 
       - name: Send slack notification
         slack:
           token: "{{ lookup('env','SLACK_TOKEN') }}"
           msg: '{{ ansible_date_time.time }} TEST: {{test_name}}, RESULT: {{ flag }}'
         when: slack_notify | bool and lookup('env','SLACK_TOKEN')

 
